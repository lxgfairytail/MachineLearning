{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 中文语言的文本分类技术和流程，主要包括以下几个步骤：\n",
    "（1）预处理：去除文本的噪声信息，例如HTML标签，文本格式转换，检测句子边界等等；\n",
    "（2）中文分词：使用中文分词器为文本分词，并去除停用词；\n",
    "（3）构建词向量空间：统计文本词频，生成文本的词向量空间；\n",
    "（4）权重策略--TF-IDF方法：使用TF-IDF发现特征词，并抽取为反映文档主题的特征；\n",
    "（5）分类器：使用算法训练分类器；\n",
    "（6）评价分类结果：分类器的测试结果分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.053 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文分词结束！\n"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# 中文分词\n",
    "import os\n",
    "import jieba\n",
    "\n",
    "# 定义保存文件函数\n",
    "def savefile(file, savepath):\n",
    "    f = open(savepath,'wb')\n",
    "    f.write(file).encode('utf-8')\n",
    "    f.close()\n",
    "    pass\n",
    "\n",
    "# 定义读取文件函数\n",
    "def readfile(filename):\n",
    "    fp = open(filename,'rb')\n",
    "    contents = fp.read().decode('utf-8')\n",
    "    fp.close()\n",
    "    return contents\n",
    "\n",
    "corpuses_path = \"corpuses/\"\n",
    "corpuses_path_seg = \"corpuses_seg/\"\n",
    "\n",
    "# 获取类别名称（）\n",
    "category_list = os.listdir(corpuses_path)\n",
    "\n",
    "for category in category_list:\n",
    "    category_path = corpuses_path + category + '/'         # 每一个类别的路径\n",
    "    category_path_seg = corpuses_path_seg + category + '/' # 拼出分词后每个类的存放路径\n",
    "    if not os.path.exists(category_path_seg):\n",
    "        os.makedirs(category_path_seg)\n",
    "    \n",
    "    file_name = os.listdir(category_path)                  # 获取每一类中的所有文件名\n",
    "    \n",
    "    for each_file_name in file_name:\n",
    "        full_name = category_path + each_file_name         # 文件的全路径\n",
    "        \n",
    "        contents = readfile(full_name).strip()            # 文件的内容\n",
    "        \n",
    "        # 将多余空行替换为空\n",
    "        contents = contents.replace('\\r\\n'.encode('utf-8'),\"\".encode('utf-8'))\n",
    "        \n",
    "        # 使用结巴分词\n",
    "        contents_seg = jieba.cut(contents)\n",
    "        \n",
    "        # 保存分词后的文件内容\n",
    "        savefile(\" \".join(contents_seg).encode('utf-8'),category_path_seg + each_file_name)\n",
    "    \n",
    "print(\"中文分词结束！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_name: ['C000008', 'C000010', 'C000013', 'C000014', 'C000016', 'C000020', 'C000022', 'C000023', 'C000024']\n",
      "label: ['C000008', 'C000008', 'C000008', 'C000008', 'C000008', 'C000008', 'C000008', 'C000008', 'C000008', 'C000008'] 文本语句数量： 17910\n",
      "contents: 券 通 ： 百联  文本语句数量： 17910\n",
      "filepath: corpuses_seg/C000008/100.txt\n",
      "构建文本对象结束！！！\n"
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "# 使用 picplk 模块使数据的持久化\n",
    "# 引入sklearn 的 Bunch 数据结构\n",
    "from sklearn.datasets.base import Bunch\n",
    "import os \n",
    "import pickle\n",
    "\n",
    "def savefile(file_contents,file_path):\n",
    "    fp = open(file_path,'wb')\n",
    "    fp.write(file_contents).encode('utf-8')\n",
    "    fp.close()\n",
    "    pass\n",
    "\n",
    "def readfile(file_path):\n",
    "    fp = open(file_path,'rb')\n",
    "    contents = fp.read().decode('utf-8')\n",
    "    fp.close()\n",
    "    return contents\n",
    "\n",
    "# Bunch类提供一种key,value的对象形式\n",
    "# target_name:  所有分类集名称列表\n",
    "# label:       每个文件的分类标签列表\n",
    "# filepath:    文件路径\n",
    "# contents:    分词后文件词向量形式\n",
    "bunch = Bunch(target_name = [],label =[],filepath = [],contents = [])\n",
    "\n",
    "corpuses_bag_path = 'corpuses_bag/corpuses_bag.dat'   # 持久化数据存储路径\n",
    "corpuses_path_seg = \"corpuses_seg/\"                   # 分词后数据存放文件夹\n",
    "\n",
    "# 获取类别名称（）\n",
    "category_list = os.listdir(corpuses_path_seg)\n",
    "# 类别名称，总共用共多少类\n",
    "bunch.target_name.extend(category_list)\n",
    "\n",
    "for category in category_list:\n",
    "    category_path_seg = corpuses_path_seg + category + '/'\n",
    "    \n",
    "    file_list = os.listdir(category_path_seg)\n",
    "    \n",
    "    for each_file in file_list:\n",
    "        full_name = category_path_seg + each_file\n",
    "        bunch.label.append(category)\n",
    "        bunch.filepath.append(full_name)\n",
    "        bunch.contents.append(readfile(full_name).strip())\n",
    "        \n",
    "# 数据持久化\n",
    "file_obj = open(corpuses_bag_path,'wb')\n",
    "pickle.dump(bunch,file_obj)\n",
    "file_obj.close()\n",
    "\n",
    "print('target_name:',bunch.target_name)\n",
    "print('label:',bunch.label[5:15],'文本语句数量：',len(bunch.label))\n",
    "print('contents:',bunch.contents[1][1:10],'文本语句数量：',len(bunch.contents))\n",
    "print('filepath:',bunch.filepath[1])\n",
    "print(\"构建文本对象结束！！！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据大小 13432 13432\n",
      "测试数据大小 4478 4478\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################\n",
    "# 权重策略：TF-IDF方法（Tf–idf term weighting） 进行特征抽取\n",
    "\n",
    "# 词频（term frequency，TF）指的是某一个给定的词语在该文件中出现的频率。\n",
    "# 逆向文件频率（inverse document frequency，IDF）是一个词语普遍重要性的度量。\n",
    "# 某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。\n",
    "# Tf表示词语频率，而tf-idf表示术语频率乘以逆文档频率：\n",
    "# tf-idf(t,d) = tf(t,d) x idf(t)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# 定义读取文件函数\n",
    "def readfile(filename):\n",
    "    fp = open(filename,'rb')\n",
    "    contents = fp.read().decode('utf-8')\n",
    "    fp.close()\n",
    "    return contents\n",
    "\n",
    "# 读取停词\n",
    "stop_words_path = 'corpuses_bag/hlt_stop_words.txt'\n",
    "stop_words = readfile(stop_words_path).splitlines()\n",
    "\n",
    "# 读取数据\n",
    "corpuses_bag_path = 'corpuses_bag/corpuses_bag.dat'   # 持久化数据存储路径\n",
    "fp = open(corpuses_bag_path,'rb')\n",
    "corpuses_bag = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "# 数据划分\n",
    "corpuses_bag_train,corpuses_bag_test,y_train,y_test = train_test_split(corpuses_bag.contents,corpuses_bag.label)\n",
    "print('训练数据大小',len(corpuses_bag_train),len(y_train))\n",
    "print('测试数据大小',len(corpuses_bag_test),len(y_test))\n",
    "\n",
    "# 训练数据tf-idf 特征抽取\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words, sublinear_tf = True, max_df = 0.5)\n",
    "X_train = vectorizer.fit_transform(corpuses_bag_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('发现', 74820), ('提示', 129169), ('每年', 151018), ('11', 1832), ('20', 3722), ('以前', 47351), ('教育部', 132060), ('严禁', 36520), ('企业', 48364), ('进入', 215212)]\n",
      "训练集大小： (13432, 237065)\n"
     ]
    }
   ],
   "source": [
    "X_train.vocabulary = vectorizer.vocabulary_\n",
    "print(list(X_train.vocabulary.items())[0:10])\n",
    "# 测试数据tf-idf 特征抽取\n",
    "X_test = vectorizer.transform(corpuses_bag_test)\n",
    "print(\"训练集大小：\",X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集混淆矩阵:\n",
      " [[442  26  11   0   7   5   6  11   1]\n",
      " [ 11 395  17   0  14  10   8  15   4]\n",
      " [  3   4 435   0   5   6  31  22   0]\n",
      " [  0   1   0 474   4   1   2   5   1]\n",
      " [  0   9   4   0 451   2   5  44   3]\n",
      " [  4   4  14   0   7 393  27  45   7]\n",
      " [  1   8  10   1   1  13 421  34   0]\n",
      " [  1   3   8   0  11  13   8 463   5]\n",
      " [  0   2   2   0   0   2   0  36 439]]\n",
      "测试集准确率： 0.873827601608\n",
      "测试集分类报告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    C000008       0.96      0.87      0.91       509\n",
      "    C000010       0.87      0.83      0.85       474\n",
      "    C000013       0.87      0.86      0.86       506\n",
      "    C000014       1.00      0.97      0.98       488\n",
      "    C000016       0.90      0.87      0.89       518\n",
      "    C000020       0.88      0.78      0.83       501\n",
      "    C000022       0.83      0.86      0.84       489\n",
      "    C000023       0.69      0.90      0.78       512\n",
      "    C000024       0.95      0.91      0.93       481\n",
      "\n",
      "avg / total       0.88      0.87      0.88      4478\n",
      "\n",
      "训练集分类报告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    C000008       0.99      0.97      0.98      1481\n",
      "    C000010       0.98      0.98      0.98      1516\n",
      "    C000013       0.99      0.99      0.99      1484\n",
      "    C000014       1.00      1.00      1.00      1502\n",
      "    C000016       0.99      1.00      1.00      1472\n",
      "    C000020       0.99      0.99      0.99      1489\n",
      "    C000022       0.99      1.00      0.99      1501\n",
      "    C000023       1.00      0.99      1.00      1478\n",
      "    C000024       1.00      1.00      1.00      1509\n",
      "\n",
      "avg / total       0.99      0.99      0.99     13432\n",
      "\n",
      "训练集准确率： 0.991140559857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "MulNB = MultinomialNB(alpha = 0.0001)\n",
    "MulNB.fit(X_train,y_train)\n",
    "predicted = MulNB.predict(X_test)\n",
    "print('测试集混淆矩阵:\\n',confusion_matrix(y_test,predicted))\n",
    "print(\"测试集准确率：\",MulNB.score(X_test,y_test))\n",
    "print('测试集分类报告：\\n',classification_report(y_test,predicted))\n",
    "print('训练集分类报告：\\n',classification_report(y_train,MulNB.predict(X_train)))\n",
    "print(\"训练集准确率：\",MulNB.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['*', '一一', '~~~~', '’', '. ', '『', '.一', './', '-- ', '』']\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################################\n",
    "###################    停用词\n",
    "# 由于文本在存储为向量空间时，维度比较高。为节省存储空间和提高搜索效率，在文本分类之前会自动\n",
    "# 过滤掉某些字或词，这些字或词即被称为停用词\n",
    "# 从这个网址下载停用词表：http://www.threedweb.cn/thread-1294-1-1.html\n",
    "# 读取停用词列表\n",
    "# 读取文件 \n",
    "# 1. 读取停用词表 \n",
    "# 读取文件\n",
    "\n",
    "# 定义读取文件函数\n",
    "def readfile(filename):\n",
    "    fp = open(filename,'rb')\n",
    "    contents = fp.read().decode('utf-8')\n",
    "    fp.close()\n",
    "    return contents\n",
    "\n",
    "stop_words_path = 'corpuses_bag/hlt_stop_words.txt'\n",
    "\n",
    "#  按照行('\\r', '\\r\\n', \\n')分隔，返回一个包含各行作为元素的列表，\n",
    "#  str.splitlines([keepends])如果参数 keepends 为 False，不包含换行符，如果为 True，则保留换行符。\n",
    "stop_words = readfile(stop_words_path).splitlines()\n",
    "\n",
    "print(stop_words[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "# 朴素贝叶斯实现，朴素贝叶斯分类的流程可以表示如下：\n",
    "# 第一阶段：训练数据生成训练样本集：TF-IDF\n",
    "# 第二阶段：对每个类别计算P(yi)\n",
    "# 第三阶段：对每个特征属性计算所有划分的条件概率\n",
    "# 第四阶段：对每个类别计算(P(x|yi ) P(yi)\n",
    "# 第五阶段：以(P(x|yi ) P(yi)的最大项作为x 的所属类别\n",
    "import numpy as np\n",
    "import copy \n",
    "\n",
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him','my'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec\n",
    "\n",
    "class Naive_bayes:\n",
    "    def __init__(self):\n",
    "        self.vocabulary = []     # 词典典\n",
    "        self.vocabulary_length = 0        # 词典词长\n",
    "        self.tf = 0              # 词频\n",
    "        self.idf = 0             # 逆文档频率\n",
    "        self.tf_idf = 0          # 词频-逆文档频率\n",
    "        self.tdm = []            # 条件概率P(x|yi)\n",
    "        self.doc_numbers = 0      # 文档长度\n",
    "        self.labels = []         #文档标签\n",
    "        self.pcategory = {}      # 每一类概率 P(yi)\n",
    "     \n",
    "    # 导入和训练数据集，生成算法必须的参数和数据结构：\n",
    "    def train_data(self,traindoc,labels):\n",
    "        self.doc_numbers = len(traindoc)\n",
    "        tempvocabulary = set()\n",
    "        [tempvocabulary.add(each_word) for each_doc in traindoc for each_word in each_doc]  \n",
    "        \n",
    "        self.vocabulary = list(tempvocabulary)\n",
    "        \n",
    "        self.vocabulary_length = len(self.vocabulary)\n",
    "        \n",
    "        self._calc_pcategory(labels)\n",
    "        \n",
    "        self._calc_word_frequence(traindoc)\n",
    "        \n",
    "        self._build_tdm()\n",
    "        \n",
    "    # 计算每个类的概率P(yi)\n",
    "    def _calc_pcategory(self,labels):\n",
    "        self.labels = labels\n",
    "        temp_labels = set(self.labels)\n",
    "        \n",
    "        for label in temp_labels:\n",
    "            self.pcategory[label] = float(self.labels.count(label))/float(len(self.labels))\n",
    "    \n",
    "    # 计算词频 tf-idf 权重方法\n",
    "    def _calc_word_frequence(self,traindoc):\n",
    "        self.tf = np.zeros((self.doc_numbers,self.vocabulary_length))\n",
    "        self.idf = np.zeros(self.vocabulary_length)\n",
    "        self.tf_idf = np.zeros((self.doc_numbers,self.vocabulary_length))\n",
    "        for doc_index in range(self.doc_numbers):\n",
    "            # 计算词频 tf\n",
    "            for vocabulary_index in range(self.vocabulary_length):\n",
    "                self.tf[doc_index,vocabulary_index] = float(traindoc[doc_index].count(self.vocabulary[vocabulary_index]) + 0.01)\\\n",
    "                                                            / float(len(traindoc[doc_index]))\n",
    "            # 计算逆文档频率\n",
    "            for each_word in set(traindoc[doc_index]):\n",
    "                self.idf[self.vocabulary.index(each_word)] += 1\n",
    "                \n",
    "        self.idf = np.log(float(self.doc_numbers)/self.idf)\n",
    "        self.tf_idf = self.tf * self.idf\n",
    "    \n",
    "    # 计算条件概率 P(X|yi)\n",
    "    def _build_tdm(self):\n",
    "        # 统计条件概率\n",
    "        self.tdm = np.zeros((len(self.pcategory),self.vocabulary_length))\n",
    "        # 统计每个分类的词汇总数\n",
    "        each_category_total_vocabulary = np.zeros((len(self.pcategory),1))\n",
    "        for doc_index in range(self.doc_numbers):\n",
    "            # 将同一类别的词向量空间值加总\n",
    "            self.tdm[self.labels[doc_index]] += self.tf_idf[doc_index]\n",
    "            # 计算每一类的总词汇数\n",
    "            each_category_total_vocabulary[self.labels[doc_index]] = np.sum(self.tdm[self.labels[doc_index]])\n",
    "        # 计算条件概率\n",
    "        self.tdm = self.tdm / each_category_total_vocabulary\n",
    "        \n",
    "    def predict(self,testdoc):\n",
    "        # 存储测试集词汇出现次数\n",
    "        self.testset = np.zeros((len(testdoc),self.vocabulary_length))\n",
    "        self.predict_labels = []\n",
    "        \n",
    "        # 计算测试集词汇出现次数\n",
    "        for test_doc_index in range(len(testdoc)):\n",
    "            for each_word in testdoc[test_doc_index]:\n",
    "                if each_word in self.vocabulary:\n",
    "                    self.testset[test_doc_index,self.vocabulary.index(each_word)] += 1\n",
    "        \n",
    "        for test_set_index in range(len(self.testset)):\n",
    "            predict_value = -float('Inf')\n",
    "            predict_class = ''\n",
    "            for tdm_vect,keyclass in zip(self.tdm,self.pcategory):\n",
    "                temp_value = np.sum(self.testset[test_set_index] * np.log(tdm_vect)) + np.log(self.pcategory[keyclass])\n",
    "                if temp_value > predict_value:\n",
    "                    predict_value = copy.deepcopy(temp_value)\n",
    "                    predict_class = copy.deepcopy(keyclass)\n",
    "            self.predict_labels.append(predict_class)\n",
    "        \n",
    "        return self.predict_labels\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词典词汇为 ['dog', 'worthless', 'has', 'cute', 'please', 'dalmation', 'maybe', 'him', 'licks', 'how', 'ate', 'help', 'buying', 'my', 'steak', 'stop', 'food', 'take', 'stupid', 'love', 'not', 'problems', 'mr', 'flea', 'so', 'to', 'posting', 'quit', 'is', 'I', 'garbage', 'park']\n",
      "训练类别标签 [0, 1, 0, 1, 0, 1]\n",
      "第一条文档的词频逆文档频率 [ 0.10001124  0.00156945  0.25852529  0.00255966  0.25852529  0.00255966\n",
      "  0.00255966  0.00099021  0.00255966  0.00255966  0.00255966  0.25852529\n",
      "  0.00255966  0.10001124  0.00255966  0.00156945  0.00255966  0.00255966\n",
      "  0.00099021  0.00255966  0.00255966  0.25852529  0.00255966  0.25852529\n",
      "  0.00255966  0.00156945  0.00255966  0.00255966  0.00255966  0.00255966\n",
      "  0.00255966  0.00255966]\n",
      "条件概率 [[ 0.02265408  0.00089473  0.05855996  0.04587091  0.05855996  0.04587091\n",
      "   0.00145924  0.03492607  0.04587091  0.04587091  0.04587091  0.05855996\n",
      "   0.00145924  0.07419642  0.04587091  0.02812562  0.00145924  0.00145924\n",
      "   0.00056451  0.04587091  0.00145924  0.05855996  0.04587091  0.05855996\n",
      "   0.04587091  0.02812562  0.00145924  0.00145924  0.04587091  0.04587091\n",
      "   0.00145924  0.00145924]\n",
      " [ 0.04956002  0.09841469  0.00212378  0.00212378  0.00212378  0.00212378\n",
      "   0.05611821  0.02170949  0.00212378  0.00212378  0.00212378  0.00212378\n",
      "   0.07411636  0.00082159  0.00212378  0.05427265  0.07411636  0.05611821\n",
      "   0.08298066  0.00212378  0.05611821  0.00212378  0.00212378  0.00212378\n",
      "   0.00212378  0.03440873  0.08851487  0.07411636  0.00212378  0.00212378\n",
      "   0.08851487  0.05611821]]\n",
      "类别概率 {0: 0.5, 1: 0.5}\n"
     ]
    }
   ],
   "source": [
    "naive_bayes = Naive_bayes()\n",
    "traindata, labels = loadDataSet()\n",
    "naive_bayes.train_data(traindata,labels)\n",
    "print('词典词汇为',naive_bayes.vocabulary)\n",
    "print('训练类别标签',naive_bayes.labels)\n",
    "print('第一条文档的词频逆文档频率',naive_bayes.tf_idf[0])\n",
    "print('条件概率',naive_bayes.tdm)\n",
    "print('类别概率',naive_bayes.pcategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "predict = naive_bayes.predict(traindata)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
