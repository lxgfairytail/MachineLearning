{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 中文语言的文本分类技术和流程，主要包括以下几个步骤：\n",
    "（1）预处理：去除文本的噪声信息，例如HTML标签，文本格式转换，检测句子边界等等；\n",
    "（2）中文分词：使用中文分词器为文本分词，并去除停用词；\n",
    "（3）构建词向量空间：统计文本词频，生成文本的词向量空间；\n",
    "（4）权重策略--TF-IDF方法：使用TF-IDF发现特征词，并抽取为反映文档主题的特征；\n",
    "（5）分类器：使用算法训练分类器；\n",
    "（6）评价分类结果：分类器的测试结果分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文分词结束！\n"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# 中文分词\n",
    "import os\n",
    "import jieba\n",
    "\n",
    "# 定义保存文件函数\n",
    "def savefile(file, savepath):\n",
    "    f = open(savepath,'wb')\n",
    "    f.write(file)\n",
    "    f.close()\n",
    "    pass\n",
    "\n",
    "# 定义读取文件函数\n",
    "def readfile(filename):\n",
    "    fp = open(filename,'rb')\n",
    "    contents = fp.read()\n",
    "    fp.close()\n",
    "    return contents\n",
    "\n",
    "corpuses_path = \"corpuses/\"\n",
    "corpuses_path_seg = \"corpuses_seg/\"\n",
    "\n",
    "# 获取类别名称（）\n",
    "category_list = os.listdir(corpuses_path)\n",
    "\n",
    "for category in category_list:\n",
    "    category_path = corpuses_path + category + '/'         # 每一个类别的路径\n",
    "    category_path_seg = corpuses_path_seg + category + '/' # 拼出分词后每个类的存放路径\n",
    "    if not os.path.exists(category_path_seg):\n",
    "        os.makedirs(category_path_seg)\n",
    "    \n",
    "    file_name = os.listdir(category_path)                  # 获取每一类中的所有文件名\n",
    "    \n",
    "    for each_file_name in file_name:\n",
    "        full_name = category_path + each_file_name         # 文件的全路径\n",
    "        \n",
    "        contents = readfile(full_name).strip()            # 文件的内容\n",
    "        \n",
    "        # 将多余空行替换为空\n",
    "        contents = contents.replace('\\r\\n'.encode('utf-8'),\"\".encode('utf-8'))\n",
    "        \n",
    "        # 使用结巴分词\n",
    "        contents_seg = jieba.cut(contents)\n",
    "        \n",
    "        # 保存分词后的文件内容\n",
    "        savefile(\" \".join(contents_seg).encode('utf-8'),category_path_seg + each_file_name)\n",
    "    \n",
    "print(\"中文分词结束！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_name: ['C000008', 'C000010', 'C000013', 'C000014', 'C000016', 'C000020', 'C000022', 'C000023', 'C000024']\n",
      "label: ['C000008', 'C000008', 'C000008', 'C000008', 'C000008', 'C000008', 'C000008', 'C000008', 'C000008', 'C000008'] 文本语句数量： 17910\n",
      "contents: 券 通 ： 百联  文本语句数量： 17910\n",
      "filepath: corpuses_seg/C000008/100.txt\n",
      "构建文本对象结束！！！\n"
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "# 使用 picplk 模块使数据的持久化\n",
    "# 引入sklearn 的 Bunch 数据结构\n",
    "from sklearn.datasets.base import Bunch\n",
    "import os \n",
    "import pickle\n",
    "\n",
    "def savefile(file_contents,file_path):\n",
    "    fp = open(file_path,'wb')\n",
    "    fp.write(file_contents).encode('utf-8')\n",
    "    fp.close()\n",
    "    pass\n",
    "\n",
    "def readfile(file_path):\n",
    "    fp = open(file_path,'rb')\n",
    "    contents = fp.read().decode('utf-8')\n",
    "    fp.close()\n",
    "    return contents\n",
    "\n",
    "# Bunch类提供一种key,value的对象形式\n",
    "# target_name:  所有分类集名称列表\n",
    "# label:       每个文件的分类标签列表\n",
    "# filepath:    文件路径\n",
    "# contents:    分词后文件词向量形式\n",
    "bunch = Bunch(target_name = [],label =[],filepath = [],contents = [])\n",
    "\n",
    "corpuses_bag_path = 'corpuses_bag/corpuses_bag.dat'   # 持久化数据存储路径\n",
    "corpuses_path_seg = \"corpuses_seg/\"                   # 分词后数据存放文件夹\n",
    "\n",
    "# 获取类别名称（）\n",
    "category_list = os.listdir(corpuses_path_seg)\n",
    "# 类别名称，总共用共多少类\n",
    "bunch.target_name.extend(category_list)\n",
    "\n",
    "for category in category_list:\n",
    "    category_path_seg = corpuses_path_seg + category + '/'\n",
    "    \n",
    "    file_list = os.listdir(category_path_seg)\n",
    "    \n",
    "    for each_file in file_list:\n",
    "        full_name = category_path_seg + each_file\n",
    "        bunch.label.append(category)\n",
    "        bunch.filepath.append(full_name)\n",
    "        bunch.contents.append(readfile(full_name).strip())\n",
    "        \n",
    "# 数据持久化\n",
    "file_obj = open(corpuses_bag_path,'wb')\n",
    "pickle.dump(bunch,file_obj)\n",
    "file_obj.close()\n",
    "\n",
    "print('target_name:',bunch.target_name)\n",
    "print('label:',bunch.label[5:15],'文本语句数量：',len(bunch.label))\n",
    "print('contents:',bunch.contents[1][1:10],'文本语句数量：',len(bunch.contents))\n",
    "print('filepath:',bunch.filepath[1])\n",
    "print(\"构建文本对象结束！！！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据大小 14328 14328\n",
      "测试数据大小 3582 3582\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################\n",
    "# 权重策略：TF-IDF方法（Tf–idf term weighting） 进行特征抽取\n",
    "\n",
    "# 词频（term frequency，TF）指的是某一个给定的词语在该文件中出现的频率。\n",
    "# 逆向文件频率（inverse document frequency，IDF）是一个词语普遍重要性的度量。\n",
    "# 某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。\n",
    "# Tf表示词语频率，而tf-idf表示术语频率乘以逆文档频率：\n",
    "# tf-idf(t,d) = tf(t,d) x idf(t)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# 定义读取文件函数\n",
    "def readfile(filename):\n",
    "    fp = open(filename,'rb')\n",
    "    contents = fp.read().decode('utf-8')\n",
    "    fp.close()\n",
    "    return contents\n",
    "\n",
    "# 读取停词\n",
    "stop_words_path = 'corpuses_bag/hlt_stop_words.txt'\n",
    "stop_words = readfile(stop_words_path).splitlines()\n",
    "\n",
    "# 读取数据\n",
    "corpuses_bag_path = 'corpuses_bag/corpuses_bag.dat'   # 持久化数据存储路径\n",
    "fp = open(corpuses_bag_path,'rb')\n",
    "corpuses_bag = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "# 数据划分\n",
    "corpuses_bag_train,corpuses_bag_test,y_train,y_test = train_test_split(corpuses_bag.contents,corpuses_bag.label,test_size=0.2,train_size = 0.8)\n",
    "print('训练数据大小',len(corpuses_bag_train),len(y_train))\n",
    "print('测试数据大小',len(corpuses_bag_test),len(y_test))\n",
    "\n",
    "# 训练数据tf-idf 特征抽取\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words, sublinear_tf = True, max_df = 0.5)\n",
    "X_train = vectorizer.fit_transform(corpuses_bag_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('kc', 19081), ('135', 2247), ('国际', 87862), ('在线', 88675), ('消息', 162533), ('英国', 202120), ('防务', 230731), ('新闻', 137868), ('2006', 3745), ('报道', 128034)]\n",
      "训练集大小： (14328, 243931)\n"
     ]
    }
   ],
   "source": [
    "X_train.vocabulary = vectorizer.vocabulary_\n",
    "print(list(X_train.vocabulary.items())[0:10])\n",
    "# 测试数据tf-idf 特征抽取\n",
    "X_test = vectorizer.transform(corpuses_bag_test)\n",
    "print(\"训练集大小：\",X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集混淆矩阵:\n",
      " [[324  17  11   0   9   2   6  11   3]\n",
      " [  6 324   9   1  14   7   8  19   5]\n",
      " [  6   7 339   0   6   3  26   8   1]\n",
      " [  0   1   0 388   7   0   1   2   1]\n",
      " [  1   6   5   0 362   2   6  25   2]\n",
      " [  1   4   8   0   2 341  15  42   1]\n",
      " [  0   3   4   0   2   5 325  35   1]\n",
      " [  3   2   4   0  15   9  10 372   4]\n",
      " [  0   1   1   0   0   4   0  28 359]]\n",
      "测试集准确率： 0.874930206588\n",
      "测试集分类报告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    C000008       0.95      0.85      0.90       383\n",
      "    C000010       0.89      0.82      0.85       393\n",
      "    C000013       0.89      0.86      0.87       396\n",
      "    C000014       1.00      0.97      0.98       400\n",
      "    C000016       0.87      0.89      0.88       409\n",
      "    C000020       0.91      0.82      0.87       414\n",
      "    C000022       0.82      0.87      0.84       375\n",
      "    C000023       0.69      0.89      0.77       419\n",
      "    C000024       0.95      0.91      0.93       393\n",
      "\n",
      "avg / total       0.88      0.87      0.88      3582\n",
      "\n",
      "训练集分类报告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    C000008       0.99      0.97      0.98      1607\n",
      "    C000010       0.97      0.98      0.97      1597\n",
      "    C000013       0.98      0.98      0.98      1594\n",
      "    C000014       1.00      1.00      1.00      1590\n",
      "    C000016       1.00      1.00      1.00      1581\n",
      "    C000020       0.99      0.99      0.99      1576\n",
      "    C000022       0.98      1.00      0.99      1615\n",
      "    C000023       1.00      0.99      1.00      1571\n",
      "    C000024       0.99      1.00      1.00      1597\n",
      "\n",
      "avg / total       0.99      0.99      0.99     14328\n",
      "\n",
      "训练集准确率： 0.989461194863\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "MulNB = MultinomialNB(alpha = 0.0001)\n",
    "MulNB.fit(X_train,y_train)\n",
    "predicted = MulNB.predict(X_test)\n",
    "print('测试集混淆矩阵:\\n',confusion_matrix(y_test,predicted))\n",
    "print(\"测试集准确率：\",MulNB.score(X_test,y_test))\n",
    "print('测试集分类报告：\\n',classification_report(y_test,predicted))\n",
    "print('训练集分类报告：\\n',classification_report(y_train,MulNB.predict(X_train)))\n",
    "print(\"训练集准确率：\",MulNB.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['*', '一一', '~~~~', '’', '. ', '『', '.一', './', '-- ', '』']\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################################\n",
    "###################    停用词\n",
    "# 由于文本在存储为向量空间时，维度比较高。为节省存储空间和提高搜索效率，在文本分类之前会自动\n",
    "# 过滤掉某些字或词，这些字或词即被称为停用词\n",
    "# 从这个网址下载停用词表：http://www.threedweb.cn/thread-1294-1-1.html\n",
    "# 读取停用词列表\n",
    "# 读取文件 \n",
    "# 1. 读取停用词表 \n",
    "# 读取文件\n",
    "\n",
    "# 定义读取文件函数\n",
    "def readfile(filename):\n",
    "    fp = open(filename,'rb')\n",
    "    contents = fp.read().decode('utf-8')\n",
    "    fp.close()\n",
    "    return contents\n",
    "\n",
    "stop_words_path = 'corpuses_bag/hlt_stop_words.txt'\n",
    "\n",
    "#  按照行('\\r', '\\r\\n', \\n')分隔，返回一个包含各行作为元素的列表，\n",
    "#  str.splitlines([keepends])如果参数 keepends 为 False，不包含换行符，如果为 True，则保留换行符。\n",
    "stop_words = readfile(stop_words_path).splitlines()\n",
    "\n",
    "print(stop_words[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
