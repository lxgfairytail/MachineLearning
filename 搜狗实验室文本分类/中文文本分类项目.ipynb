{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 中文语言的文本分类技术和流程，主要包括以下几个步骤：\n",
    "（1）预处理：去除文本的噪声信息，例如HTML标签，文本格式转换，检测句子边界等等；\n",
    "（2）中文分词：使用中文分词器为文本分词，并去除停用词；\n",
    "（3）构建词向量空间：统计文本词频，生成文本的词向量空间；\n",
    "（4）权重策略--TF-IDF方法：使用TF-IDF发现特征词，并抽取为反映文档主题的特征；\n",
    "（5）分类器：使用算法训练分类器；\n",
    "（6）评价分类结果：分类器的测试结果分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "################# 文本预处理\n",
    "\n",
    "# 1.选择处理的文本的范围，2.建立分类文本语料库（训练集和测试集）\n",
    "# 3.文本格式转换（不同格式的文本不论何种处理形式都要统一转换为纯文本文件）\n",
    "# 例如：使用lxml库去除html标签\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys  \n",
    "import os\n",
    "import time\n",
    "from lxml import etree,html\n",
    "\n",
    "# htm文件路径，以及读取文件\n",
    "path = \"1.htm\"\n",
    "content = open(path,\"rb\").read()\n",
    "page = html.document_fromstring(content) # 解析文件\n",
    "text = page.text_content() # 去除所有标签\n",
    "#print(text)              # 输出去除标签后解析结果\n",
    "\n",
    "# 4.检测句子边界：标记句子的结束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jieba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1b2296dbc4c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;31m#默认模式\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jieba'"
     ]
    }
   ],
   "source": [
    "#####################################################################################\n",
    "################ 中文分词介绍\n",
    "\n",
    "#jieba分词（linux环境），它是专门使用Python语言开发的分词系统\n",
    "import sys  \n",
    "import os\n",
    "import jieba\n",
    "\n",
    "#默认模式\n",
    "seg_list = jieba.cut(\"小明1995年毕业于北京清华大学\", cut_all=False)\n",
    "print(\"Default Mode:\", \" \".join(seg_list))  \n",
    "\n",
    "seg_list = jieba.cut(\"小明1995年毕业于北京清华大学\")\n",
    "print(\"  \".join(seg_list))\n",
    "\n",
    "#全模式\n",
    "seg_list = jieba.cut(\"小明1995年毕业于北京清华大学\", cut_all=True)\n",
    "print(\"Full Mode:\", \"/ \".join(seg_list))  \n",
    "\n",
    "#搜索引擎模式\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")  \n",
    "print(\"/  \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文语料分词结束！！！\n"
     ]
    }
   ],
   "source": [
    "#######################################################################################\n",
    "#对中文语料库进行分词\n",
    "\n",
    "#设置字符集，并导入jieba分词包\n",
    "import os \n",
    "import jieba\n",
    "\n",
    "#定义创建两个函数，用于处理读取和保存文件：\n",
    "#保存至文件\n",
    "def savefile(savepath,content):\n",
    "    fp = open(savepath,\"wb\")\n",
    "    fp.write(content)\n",
    "    fp.close()\n",
    "\n",
    "#读取文件\n",
    "def readfile(path):\n",
    "    fp = open(path,\"rb\")\n",
    "    content = fp.read()\n",
    "    fp.close()\n",
    "    return content\n",
    "\n",
    "corpus_path = \"train_corpus_small/\"  # 未分词分类语料库路径\n",
    "seg_path = \"train_corpus_seg/\"      # 分词后分类语料库路径\n",
    "\n",
    "catelist = os.listdir(corpus_path)  # 获取corpus_path下的所有子目录\n",
    "\n",
    "#接上面的部分，以下是整个语料库的分词主程序：\n",
    "#获取每个目录下所有的文件\n",
    "for mydir in catelist:\n",
    "    class_path = corpus_path+mydir+\"/\"                 # 拼出分类子目录的路径\n",
    "    seg_dir = seg_path+mydir+\"/\"                       # 拼出分词后语料分类目录\n",
    "    if not os.path.exists(seg_dir):                    # 是否存在目录，如果没有创建\n",
    "            os.makedirs(seg_dir)\n",
    "    file_list = os.listdir(class_path)                 # 获取class_path下的所有文件\n",
    "    \n",
    "    for file_path in file_list:                       # 遍历类别目录下文件\n",
    "        fullname = class_path + file_path             # 拼出文件名全路径\n",
    "        content = readfile(fullname).strip()          # 读取文件内容\n",
    "        content = content.replace(\"\\r\\n\".encode('utf-8'),\"\".encode('utf-8'))          # 删除换行和多余的空格\n",
    "        content_seg = jieba.cut(content.strip())      # 为文件内容分词\n",
    "        savefile(seg_dir + file_path,\" \".join(content_seg).encode('utf-8'))  # 将处理后的文件保存到分词后语料目录\n",
    "\n",
    "print(\"中文语料分词结束！！！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_name: ['art', 'computer', 'economic', 'education', 'environment', 'medical', 'military', 'politics', 'sports', 'traffic']\n",
      "label: ['art', 'art', 'art', 'art', 'art', 'art', 'computer', 'computer', 'computer', 'computer']\n",
      "contents: ﻿ 唐建平 出处 ：\n",
      " test_corpus_seg/art/2160.txt\n",
      "构建文本对象结束！！！\n"
     ]
    }
   ],
   "source": [
    "#######################################################################################################\n",
    "# 在实际应用中，为了后续生成向量空间模型的方便，这些分词后的文本信息还要转换为文本向量信息并对象化。\n",
    "# 这里需要引入一个Scikit-Learning库的Bunch数据结构：\n",
    "import sys  \n",
    "import os \n",
    "import pickle\n",
    "from sklearn.datasets.base import Bunch\n",
    "\n",
    "# 保存至文件\n",
    "def savefile(savepath,content):\n",
    "    fp = open(savepath,\"wb\")\n",
    "    fp.write(content).encode('utf-8')\n",
    "    fp.close()\n",
    "\n",
    "# 读取文件\t\n",
    "def readfile(path):\n",
    "    fp = open(path,\"rb\")\n",
    "    content = fp.read().decode('utf-8')\n",
    "    fp.close()\n",
    "    return content\n",
    "\n",
    "\n",
    "# Bunch类提供一种key,value的对象形式\n",
    "# target_name: 所有分类集名称列表\n",
    "# label:       每个文件的分类标签列表\n",
    "# filenames:   文件路径\n",
    "# contents:    分词后文件词向量形式\n",
    "bunch = Bunch(target_name=[],label=[],filenames=[],contents=[])\t\n",
    "\n",
    "wordbag_path = \"test_word_bag/test_set.dat\"  # 持久化分词分类语料库路径\n",
    "seg_path = \"test_corpus_seg/\"                # 分词后分类语料库路径\n",
    "\n",
    "\n",
    "catelist = os.listdir(seg_path)              # 获取seg_path下的所有子目录\n",
    "bunch.target_name.extend(catelist)\n",
    "# 获取每个目录下所有的文件\n",
    "for mydir in catelist:\n",
    "    class_path = seg_path+mydir+\"/\"            # 拼出分类子目录的路径\n",
    "    file_list = os.listdir(class_path)         # 获取class_path下的所有文件\n",
    "    for file_path in file_list:               # 遍历类别目录下文件\n",
    "        fullname = class_path + file_path      # 拼出文件名全路径\n",
    "        bunch.label.append(mydir)\n",
    "        bunch.filenames.append(fullname)\n",
    "        bunch.contents.append(readfile(fullname).strip())  # 读取文件内容\n",
    "# 函数原型\n",
    "# 声明：s为字符串，rm为要删除的字符序列\n",
    "# s.strip(rm)   删除s字符串中开头、结尾处，位于rm删除序列的字符\n",
    "# s.lstrip(rm)  删除s字符串中开头处，位于 rm删除序列的字符\n",
    "# s.rstrip(rm)  删除s字符串中结尾处，位于 rm删除序列的字符\n",
    "# 当rm为空时，默认删除空白符（包括'\\n', '\\r',  '\\t',  ' ')\n",
    "\n",
    "#对象持久化                                                                                              \n",
    "file_obj = open(wordbag_path, \"wb\")\n",
    "pickle.dump(bunch,file_obj)                      \n",
    "file_obj.close()\n",
    "\n",
    "print('target_name:',bunch.target_name)\n",
    "print('label:',bunch.label[5:15])\n",
    "print('contents:',bunch.contents[1][0:10])\n",
    "print('',bunch.filenames[1])\n",
    "print(\"构建文本对象结束！！！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['*', '一一', '~~~~', '’', '. ', '『', '.一', './', '-- ', '』']\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################################\n",
    "###################    停用词\n",
    "# 由于文本在存储为向量空间时，维度比较高。为节省存储空间和提高搜索效率，在文本分类之前会自动\n",
    "# 过滤掉某些字或词，这些字或词即被称为停用词\n",
    "# 从这个网址下载停用词表：http://www.threedweb.cn/thread-1294-1-1.html\n",
    "# 读取停用词列表\n",
    "# 读取文件 \n",
    "# 1. 读取停用词表 \n",
    "# 读取文件\t\n",
    "\n",
    "stopword_path = \"train_word_bag/hlt_stop_words.txt\" \n",
    "# Python splitlines() 按照行('\\r', '\\r\\n', \\n')分隔，返回一个包含各行作为元素的列表，\n",
    "# 如果参数 keepends 为 False，不包含换行符，如果为 True，则保留换行符。\n",
    "stpwrdlst = readfile(stopword_path).splitlines()\n",
    "print(stpwrdlst[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练词频稀疏矩阵: [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "训练词汇字典: [('19960518', 322), ('努力', 8526), ('创作', 8087), ('无愧于', 16791), ('伟大', 6119), ('时代', 16942), ('艺术', 23812), ('精品', 22397), ('刘忠德', 8064), ('艺术创作', 23816)]\n",
      "if-idf词向量空间创建成功！！！\n"
     ]
    }
   ],
   "source": [
    "#############################################################################################################\n",
    "# 权重策略：TF-IDF方法（Tf–idf term weighting）\n",
    "# 在一个大的文本语料库中，一些单词将会非常的出现（例如“the”，“a”，“is”是英文），因此对文档的实际内容没有\n",
    "# 任何有意义的信息.如果我们将直接计数数据直接提供给分类器，那么非常频繁的词语会影向有意义词语的频率。\n",
    "# 为了将计数特征重新调整为适合分类器使用的浮点值，使用tf-idf变换是非常常见的。\n",
    "\n",
    "# 词频（term frequency，TF）指的是某一个给定的词语在该文件中出现的频率。\n",
    "# 逆向文件频率（inverse document frequency，IDF）是一个词语普遍重要性的度量。\n",
    "# 某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。\n",
    "\n",
    "# Tf表示词语频率，而tf-idf表示术语频率乘以逆文档频率：\n",
    "# tf-idf(t,d) = tf(t,d) x idf(t)\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "import sys  \n",
    "import os \n",
    "#引入Bunch类\n",
    "from sklearn.datasets.base import Bunch\n",
    "#引入持久化类\n",
    "import pickle\n",
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "# 读取文件\n",
    "def readfile(path):\n",
    "    fp = open(path,\"rb\")\n",
    "    content = fp.read()\n",
    "    fp.close()\n",
    "    return content\n",
    "\n",
    "#计算训练语料的tf-idf权值并持久化为词袋\n",
    "\n",
    "#读取bunch对象\n",
    "def readbunchobj(path):\n",
    "    file_obj = open(path, \"rb\")\n",
    "    bunch = pickle.load(file_obj)\n",
    "    file_obj.close()\n",
    "    return bunch\n",
    "#写入bunch对象\t\n",
    "def writebunchobj(path,bunchobj):\n",
    "    file_obj = open(path, \"wb\")\n",
    "    pickle.dump(bunchobj,file_obj) \n",
    "    file_obj.close()\n",
    "\n",
    "# 1. 读取停用词表\t\n",
    "stopword_path = \"train_word_bag/hlt_stop_words.txt\"\n",
    "stpwrdlst = readfile(stopword_path).splitlines()\n",
    "\n",
    "# 2. 导入分词后的词向量bunch对象\n",
    "path = \"test_word_bag/test_set.dat\"        # 词向量空间保存路径\n",
    "bunch = readbunchobj(path)\n",
    "\n",
    "# 3. 构建测试集tf-idf向量空间\n",
    "tfidfspace = Bunch(target_name=bunch.target_name,label=bunch.label,filenames=bunch.filenames, tdm=[], vocabulary={})\n",
    "\n",
    "# 4. 使用TfidfVectorizer初始化向量空间模型 \n",
    "#    TfidfVectorizer的类，它将CountVectorizer和TfidfTransformer的所有选项组合在一个模型中\n",
    "vectorizer = TfidfVectorizer(stop_words=stpwrdlst,sublinear_tf = True,max_df = 0.5)\n",
    "# 该类会统计每个词语的tf-idf权值\n",
    "transformer = TfidfTransformer() \n",
    "\n",
    "#5. 文本转为词频矩阵,单独保存字典文件 \n",
    "tfidfspace.tdm = vectorizer.fit_transform(bunch.contents)\n",
    "tfidfspace.vocabulary = vectorizer.vocabulary_\n",
    "print('训练词频稀疏矩阵:',tfidfspace.tdm.toarray()[0:5])\n",
    "print('训练词汇字典:',list(tfidfspace.vocabulary.items())[0:10])\n",
    "\n",
    "#6. 创建词袋的持久化\n",
    "space_path = \"train_word_bag/tfdifspace.dat\"        # 词向量空间保存路径\n",
    "writebunchobj(space_path,tfidfspace)\n",
    "\n",
    "print(\"if-idf词向量空间创建成功！！！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试词频稀疏矩阵: [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "测试词汇字典: [('19960518', 322), ('努力', 8526), ('创作', 8087), ('无愧于', 16791), ('伟大', 6119), ('时代', 16942), ('艺术', 23812), ('精品', 22397), ('刘忠德', 8064), ('艺术创作', 23816)]\n",
      "测试词汇字典: [('19960518', 322), ('努力', 8526), ('创作', 8087), ('无愧于', 16791), ('伟大', 6119), ('时代', 16942), ('艺术', 23812), ('精品', 22397), ('刘忠德', 8064), ('艺术创作', 23816)]\n",
      "test词向量空间创建成功！！！\n"
     ]
    }
   ],
   "source": [
    "# 训练步骤与训练集相同，首先是分词，之后生成文件词向量文件，直至生成词向量模型。\n",
    "# 不同的是在训练词向量模型时，需要加载训练集词袋，将测试集产生的词向量映射到训练\n",
    "# 集词袋的词典中，生成向量空间模型。\n",
    "\n",
    "import sys  \n",
    "import os \n",
    "#引入Bunch类\n",
    "from sklearn.datasets.base import Bunch\n",
    "#引入持久化类\n",
    "import pickle\n",
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "\n",
    "\n",
    "# 读取文件\n",
    "def readfile(path):\n",
    "    fp = open(path,\"rb\")\n",
    "    content = fp.read()\n",
    "    fp.close()\n",
    "    return content\n",
    "\n",
    "#计算训练语料的tfidf权值并持久化为词袋\n",
    "\n",
    "#读取bunch对象\n",
    "def readbunchobj(path):\n",
    "    file_obj = open(path, \"rb\")\n",
    "    bunch = pickle.load(file_obj) \n",
    "    file_obj.close()\n",
    "    return bunch\n",
    "#写入bunch对象\t\n",
    "def writebunchobj(path,bunchobj):\n",
    "    file_obj = open(path, \"wb\")\n",
    "    pickle.dump(bunchobj,file_obj) \n",
    "    file_obj.close()\n",
    "\n",
    "# 1. 读取停用词表\t\n",
    "stopword_path = \"train_word_bag/hlt_stop_words.txt\"\n",
    "stpwrdlst = readfile(stopword_path).splitlines()\n",
    "\n",
    "# 2. 导入分词后的词向量bunch对象\n",
    "path = \"test_word_bag/test_set.dat\"        # 词向量空间保存路径\n",
    "bunch = readbunchobj(path)\n",
    "\n",
    "# 3. 构建测试集tfidf向量空间\n",
    "testspace = Bunch(target_name=bunch.target_name,label=bunch.label,filenames=bunch.filenames,tdm=[],vocabulary={})\n",
    "\n",
    "# 4. 导入训练集的词袋\n",
    "trainbunch = readbunchobj(\"train_word_bag/tfdifspace.dat\")\n",
    "\n",
    "# 5. 使用TfidfVectorizer初始化向量空间模型 \n",
    "vectorizer = TfidfVectorizer(stop_words=stpwrdlst,sublinear_tf = True,max_df = 0.5,vocabulary=trainbunch.vocabulary)\n",
    "transformer=TfidfTransformer() # 该类会统计每个词语的tf-idf权值\n",
    "\n",
    "# 文本转为tf-idf矩阵,单独保存字典文件 \n",
    "testspace.tdm = vectorizer.fit_transform(bunch.contents)\n",
    "testspace.vocabulary = trainbunch.vocabulary\n",
    "print('测试词频稀疏矩阵:',testspace.tdm.toarray()[0:5])\n",
    "# 测试集词汇就是训练集词汇\n",
    "print('测试词汇字典:',list(testspace.vocabulary.items())[0:10])\n",
    "print('测试词汇字典:',list(vectorizer.vocabulary_.items())[0:10])\n",
    "\n",
    "# 创建词袋的持久化\n",
    "space_path = \"test_word_bag/testspace.dat\"        # 词向量空间保存路径\n",
    "writebunchobj(space_path,testspace)\n",
    "\n",
    "print(\"test词向量空间创建成功！！！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_corpus_seg/art/3143.txt : 实际类别: art  -->预测类别: education\n",
      "error rate: 0.9900990099009901 %\n",
      "预测完毕!!!\n",
      "精度:0.991\n",
      "召回:0.990\n",
      "f1-score:0.990\n"
     ]
    }
   ],
   "source": [
    "# 执行多项式贝叶斯算法进行测试文本分类，并返回分类精度：\n",
    "import sys  \n",
    "import os \n",
    "#引入Bunch类\n",
    "from sklearn.datasets.base import Bunch\n",
    "#引入持久化类\n",
    "import pickle\n",
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "from sklearn.naive_bayes import MultinomialNB #导入多项式贝叶斯算法\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# 读取文件\n",
    "def readfile(path):\n",
    "    fp = open(path,\"rb\")\n",
    "    content = fp.read()\n",
    "    fp.close()\n",
    "    return content\n",
    "\n",
    "#计算分类精度：\n",
    "def metrics_result(actual,predict):\n",
    "    print ('精度:{0:.3f}'.format(metrics.precision_score(actual,predict,average = 'weighted'))) \n",
    "    print ('召回:{0:0.3f}'.format(metrics.recall_score(actual,predict,average = 'weighted')))  \n",
    "    print ('f1-score:{0:.3f}'.format(metrics.f1_score(actual,predict,average = 'weighted')))  \n",
    "\n",
    "#读取bunch对象\n",
    "def readbunchobj(path):\n",
    "    file_obj = open(path, \"rb\")\n",
    "    bunch = pickle.load(file_obj) \n",
    "    file_obj.close()\n",
    "    return bunch\n",
    "\n",
    "#写入bunch对象\n",
    "def writebunchobj(path,bunchobj):\n",
    "    file_obj = open(path, \"wb\")\n",
    "    pickle.dump(bunchobj,file_obj) \n",
    "    file_obj.close()\n",
    "\n",
    "# 导入训练集\t\n",
    "trainpath = \"train_word_bag/tfdifspace.dat\"\n",
    "train_set = readbunchobj(trainpath)\n",
    "\n",
    "# 导入测试集\n",
    "testpath = \"test_word_bag/testspace.dat\"\n",
    "test_set = readbunchobj(testpath)\n",
    "# 应用朴素贝叶斯算法 \n",
    "# 1. 输入词袋向量和分类标签\n",
    "#alpha:0.001 alpha越小，迭代次数越多，精度越高\n",
    "clf = MultinomialNB(alpha = 0.001).fit(train_set.tdm, train_set.label)\n",
    "\n",
    "# 预测分类结果\n",
    "predicted = clf.predict(test_set.tdm)\n",
    "total = len(predicted);rate = 0\n",
    "for flabel,file_name,expct_cate in zip(test_set.label,test_set.filenames,predicted):\n",
    "    if flabel != expct_cate:\n",
    "        rate += 1\n",
    "        print(file_name,\": 实际类别:\",flabel,\" -->预测类别:\",expct_cate)\n",
    "# 精度\n",
    "print(\"error rate:\",float(rate)*100/float(total),\"%\")\n",
    "print(\"预测完毕!!!\")\n",
    "\n",
    "metrics_result(test_set.label,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
